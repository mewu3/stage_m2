Building DAG of jobs...
Using shell: /bin/bash
Provided cores: 12
Rules claiming more threads will be scaled down.
Conda environments: ignored
Job counts:
	count	jobs
	1	all
	1	checkHeterodimer
	1	countAllKmer1
	1	countAllKmer2
	1	evaluation2
	1	evaluation3
	1	filtering1
	1	filtering2
	1	getKmerPosition1
	1	getKmerPosition3
	1	getKmerPosition4
	1	sortAllKmer
	1	specific1
	1	specific3
	1	specific4
	15
Select jobs to execute...

[Thu Apr 29 14:42:35 2021]
rule countAllKmer1:
    input: /datater/wu/data/MSSPE-variant/zika/zika.uniq
    output: /datater/wu/data/MSSPE-variant/zika/kmer15/intermediate/allKmerCount.int
    jobid: 2
    wildcards: sample=zika

Terminating processes on user request, this might take some time.
[Thu Apr 29 14:43:37 2021]
Error in rule countAllKmer1:
    jobid: 2
    output: /datater/wu/data/MSSPE-variant/zika/kmer15/intermediate/allKmerCount.int
    shell:
        
        jellyfish count         -m 15         -s 1G         -t 12         -o /datater/wu/data/MSSPE-variant/zika/kmer15/intermediate/allKmerCount.int /datater/wu/data/MSSPE-variant/zika/zika.uniq
        
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)

Removing output files of failed job countAllKmer1 since they might be corrupted:
/datater/wu/data/MSSPE-variant/zika/kmer15/intermediate/allKmerCount.int
Complete log: /datater/wu/MSSPE/.snakemake/log/2021-04-29T144233.945740.snakemake.log
