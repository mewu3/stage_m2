Building DAG of jobs...
Using shell: /bin/bash
Provided cores: 1 (use --cores to define parallelism)
Rules claiming more threads will be scaled down.
Conda environments: ignored
Job counts:
	count	jobs
	1	MSA
	1	all
	2	checkHeterodimer
	1	evaluation1
	2	evaluation2
	2	evaluation3
	2	filtering2
	2	filtering3
	2	specific1
	2	specific3
	2	specific4
	1	splitIntoOverlappingWindows
	20
Select jobs to execute...

[Thu May  6 11:09:22 2021]
rule MSA:
    input: /datater/wu/data/MSSPE-variant/zika/zika.uniq
    output: /datater/wu/data/MSSPE-variant/zika/zika.msa
    log: /datater/wu/data/MSSPE-variant/zika/log/mafft.log
    jobid: 2
    wildcards: sample=zika

[Thu May  6 11:09:30 2021]
Finished job 2.
1 of 20 steps (5%) done
Select jobs to execute...

[Thu May  6 11:09:30 2021]
rule evaluation1:
    input: /datater/wu/data/MSSPE-variant/zika/zika.uniq, /datater/wu/data/ncbiTaxonomy/nucl_gb.accession2taxid, /datater/wu/data/ncbiTaxonomy/ncbi_lineages_2021-04-23.csv
    output: /datater/wu/data/MSSPE-variant/zika/kmer13/intermediate/aceID-taxID-species.tsv
    jobid: 17
    wildcards: sample=zika, kmerSize=kmer13

Terminating processes on user request, this might take some time.
Removing output files of failed job evaluation1 since they might be corrupted:
/datater/wu/data/MSSPE-variant/zika/kmer13/intermediate/aceID-taxID-species.tsv
Complete log: /datater/wu/MSSPE/.snakemake/log/2021-05-06T110921.541509.snakemake.log
