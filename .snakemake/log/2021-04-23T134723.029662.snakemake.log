Building DAG of jobs...
Updating job filtering2.
Updating job filtering2.
Using shell: /bin/bash
Provided cores: 12
Rules claiming more threads will be scaled down.
Conda environments: ignored
Job counts:
	count	jobs
	1	all
	2	evaluation1
	3
Select jobs to execute...

[Fri Apr 23 13:47:23 2021]
rule evaluation1:
    input: /datater/wu/data/MSSPE-basic/zika/zika.uniq, /datater/wu/data/ncbiTaxonomy/nucl_gb.accession2taxid, /datater/wu/data/ncbiTaxonomy/ncbi_lineages_2021-04-23.csv
    output: /datater/wu/data/MSSPE-basic/zika/kmer15/intermediate/aceID-taxID-species.tsv
    jobid: 21
    wildcards: sample=zika

[Fri Apr 23 13:47:23 2021]
rule evaluation1:
    input: /datater/wu/data/MSSPE-basic/enterovirus/enterovirus.uniq, /datater/wu/data/ncbiTaxonomy/nucl_gb.accession2taxid, /datater/wu/data/ncbiTaxonomy/ncbi_lineages_2021-04-23.csv
    output: /datater/wu/data/MSSPE-basic/enterovirus/kmer15/intermediate/aceID-taxID-species.tsv
    jobid: 20
    wildcards: sample=enterovirus

Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: /datater/wu/MSSPE/.snakemake/log/2021-04-23T134723.029662.snakemake.log
