Building DAG of jobs...
Updating job filtering2.
Updating job filtering2.
Using shell: /bin/bash
Provided cores: 12
Rules claiming more threads will be scaled down.
Conda environments: ignored
Job counts:
	count	jobs
	1	all
	2	allHeterodimerCheck
	3
Select jobs to execute...

[Fri Apr 23 10:04:27 2021]
rule allHeterodimerCheck:
    input: /datater/wu/data/MSSPE-basic/enterovirus/kmer15/allKmerCount.sorted.calculated.filtered.spec.txt
    output: /datater/wu/data/MSSPE-basic/enterovirus/kmer15/allOligo.set
    jobid: 18
    wildcards: sample=enterovirus

[Fri Apr 23 10:04:27 2021]
rule allHeterodimerCheck:
    input: /datater/wu/data/MSSPE-basic/zika/kmer15/allKmerCount.sorted.calculated.filtered.spec.txt
    output: /datater/wu/data/MSSPE-basic/zika/kmer15/allOligo.set
    jobid: 19
    wildcards: sample=zika

Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: /datater/wu/MSSPE/.snakemake/log/2021-04-23T100426.944594.snakemake.log
