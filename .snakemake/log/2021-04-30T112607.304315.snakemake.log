Building DAG of jobs...
Updating job filtering2.
Using shell: /bin/bash
Provided cores: 12
Rules claiming more threads will be scaled down.
Conda environments: ignored
Job counts:
	count	jobs
	1	all
	1	evaluation1
	1	evaluation2
	3
Select jobs to execute...

[Fri Apr 30 11:26:08 2021]
rule evaluation1:
    input: /datater/wu/data/MSSPE-basic/coronavirusCurated/coronavirusCurated.uniq, /datater/wu/data/ncbiTaxonomy/nucl_gb.accession2taxid, /datater/wu/data/ncbiTaxonomy/ncbi_lineages_2021-04-23.csv
    output: /datater/wu/data/MSSPE-basic/coronavirusCurated/kmer15/intermediate/aceID-taxID-species.tsv
    jobid: 11
    wildcards: sample=coronavirusCurated

Terminating processes on user request, this might take some time.
Removing output files of failed job evaluation1 since they might be corrupted:
/datater/wu/data/MSSPE-basic/coronavirusCurated/kmer15/intermediate/aceID-taxID-species.tsv
Complete log: /datater/wu/MSSPE/.snakemake/log/2021-04-30T112607.304315.snakemake.log
