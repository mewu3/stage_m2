Building DAG of jobs...
Using shell: /bin/bash
Provided cores: 12
Rules claiming more threads will be scaled down.
Conda environments: ignored
Job counts:
	count	jobs
	1	aggregate_allReverseOligo
	1	all
	1	splitFiles
	3
Select jobs to execute...

[Tue Mar 16 16:39:13 2021]
checkpoint splitFiles:
    input: /datater/wu/data_test/msa/zika.msa
    output: /datater/wu/data_test/zika/splitFiles
    jobid: 2
    wildcards: sample=zika
Downstream jobs will be updated after completion.

Updating job aggregate_allReverseOligo.
[Tue Mar 16 16:39:14 2021]
Finished job 2.
1 of 3 steps (33%) done
Select jobs to execute...

[Tue Mar 16 16:39:14 2021]
rule aggregate_allReverseOligo:
    output: /datater/wu/data_test/zika/filtering/allOligos_reverse.calculated2
    jobid: 1
    wildcards: sample=zika

Terminating processes on user request, this might take some time.
[Tue Mar 16 16:44:36 2021]
Error in rule aggregate_allReverseOligo:
    jobid: 1
    output: /datater/wu/data_test/zika/filtering/allOligos_reverse.calculated2
    shell:
        
        sed -s 1d  >> /datater/wu/data_test/zika/filtering/allOligos_reverse.calculated2
        
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)

Removing output files of failed job aggregate_allReverseOligo since they might be corrupted:
/datater/wu/data_test/zika/filtering/allOligos_reverse.calculated2
Complete log: /datater/wu/stage_m2_merge/.snakemake/log/2021-03-16T163913.333013.snakemake.log
