Building DAG of jobs...
Updating job filtering2.
Updating job filtering2.
Using shell: /bin/bash
Provided cores: 1 (use --cores to define parallelism)
Rules claiming more threads will be scaled down.
Conda environments: ignored
Job counts:
	count	jobs
	1	all
	43	calculateKmer_sort
	2	checkHeterodimer
	43	countKmer1
	43	countKmer2
	1	evaluation1
	2	evaluation2
	2	evaluation3
	43	filtering1
	1	filtering2
	1	filtering3
	1	specific1
	2	specific3
	2	specific4
	187
Select jobs to execute...

[Thu May  6 11:10:02 2021]
rule countKmer1:
    input: /datater/wu/data/MSSPE-basic/zika/splitFiles/reverse9036-9537.fasta
    output: /datater/wu/data/MSSPE-basic/zika/kmer13/intermediate/reverse9036-9537.int.kmc_pre, /datater/wu/data/MSSPE-basic/zika/kmer13/intermediate/reverse9036-9537.int.kmc_suf
    jobid: 104
    wildcards: sample=zika, kmerSize=kmer13, seg=9036-9537

Waiting at most 5 seconds for missing files.
MissingOutputException in line 1 of /datater/wu/MSSPE/rules/basic_kmerCount.smk:
Job Missing files after 5 seconds:
/datater/wu/data/MSSPE-basic/zika/kmer13/intermediate/reverse9036-9537.int.kmc_pre
/datater/wu/data/MSSPE-basic/zika/kmer13/intermediate/reverse9036-9537.int.kmc_suf
This might be due to filesystem latency. If that is the case, consider to increase the wait time with --latency-wait.
Job id: 104 completed successfully, but some output files are missing. 104
  File "/datater/wu/miniconda3/envs/snakemake/lib/python3.9/site-packages/snakemake/executors/__init__.py", line 583, in handle_job_success
  File "/datater/wu/miniconda3/envs/snakemake/lib/python3.9/site-packages/snakemake/executors/__init__.py", line 252, in handle_job_success
Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: /datater/wu/MSSPE/.snakemake/log/2021-05-06T111001.775276.snakemake.log
